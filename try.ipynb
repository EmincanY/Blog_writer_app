{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "assistant > ## Ortalama Maliyet ve Değer Üzerine: İnşaat Ödemeleri ve Birim Fiyatlandırma\n",
      "\n",
      "İnşaat sektörü, dinamik yapısı ve çeşitli bileşenleriyle karmaşık bir süreç yönetimi gerektiren bir alandır. Bu süreçte en önemli unsurlardan biri, yapının toplam maliyetini belirlemek amacıyla kullanılan 'Ortalama Maliyet ve Değer' kavramlarıdır. İnşaat ödeme süreçlerinin verimli bir şekilde yönetilmesi, hem yüklenici firmalar hem de yatırımcılar için büyük önem taşır.\n",
      "\n",
      "İnşaat projelerinde maliyet hesaplamaları yapılırken, 'Birim Fiyatlandırma' yöntemi sıkça kullanılır. Birim fiyatlandırma, her bir yapı elemanının maliyetinin belirlenmesi için kullanılır. Örneğin, metrekare başına düşen maliyet veya birim işçilik maliyeti gibi kalemler bu yöntemin kapsamındadır. Birim fiyatlandırma, toplam maliyetin daha detaylı ve doğru bir şekilde hesaplanmasını sağlar. Bu doğrultuda, projenin her aşamasında yapılan 'İnşaat Ödemeleri', bütçenin kontrol edilmesine ve maliyet tahminlerinin gerçeğe yakın olmasına yardımcı olur.\n",
      "\n",
      "Ortalama maliyetin doğru bir şekilde hesaplanması, projenin gerçek değeri ile yatırımcı beklentileri arasındaki dengeyi kurarken kritik rol oynar. Bu noktada, her bir birim fiyatının piyasa koşullarına uygunluğu ve ödemelerin zamanında yapılması, projenin finansal sürdürülebilirliği açısından büyük önem arz eder.\n",
      "\n",
      "Sonuç olarak, inşaat sektöründe maliyet ve değer dengesi, iyi yönetilen bir süreç ve doğru maliyet hesaplamaları ile elde edilebilir. İnşaat ödemeleri ve birim fiyatlandırmadaki her türlü hata, projenin genel maliyetini ve dolayısıyla piyasa değerini etkileyebilir. Bu yüzden, her iki kavramın da doğru bir şekilde anlaşılması ve uygulanması, inşaat süreçlerinin başarılı bir şekilde yönetilmesi için gereklidir."
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from typing_extensions import override\n",
    "from openai import AssistantEventHandler\n",
    "import openai\n",
    "\n",
    "# OpenAI API anahtarınızı ayarlayın\n",
    "openai.api_key = \"sk-l5-07IGyNFqRRNETnLzNBOMhmE7-tCMkZXdent5zfsT3BlbkFJgUnwnWNJwSIhcmpBBekxvCxm54hUo9QuRENJftn8YA\"\n",
    "\n",
    "# OpenAI API istemcisini başlat\n",
    "client = OpenAI(api_key=openai.api_key)\n",
    "\n",
    "# Asistanı oluştur, parametreleri belirle\n",
    "assistant = client.beta.assistants.create(\n",
    "    name=\"Content Creator\",\n",
    "    instructions=\"You are a blog post creator that writes about company topics with specific keywords.\",\n",
    "    tools=[{\"type\": \"code_interpreter\"}],  # Code interpreter ile daha dinamik içerik üretimi yapılabilir\n",
    "    model=\"gpt-4o\",\n",
    ")\n",
    "\n",
    "# Thread başlat (konuşma dizisi)\n",
    "thread = client.beta.threads.create()\n",
    "\n",
    "# Blog oluşturma için istem\n",
    "message = client.beta.threads.messages.create(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",\n",
    "    content=\"Create a 300-word blog post in Turkish about 'Average Cost and Worth' using keywords like 'Construction Payment' and 'Unit Pricing'.\",\n",
    ")\n",
    "\n",
    "# EventHandler sınıfını tanımla\n",
    "class EventHandler(AssistantEventHandler):\n",
    "    @override\n",
    "    def on_text_created(self, text) -> None:\n",
    "        print(f\"\\nassistant > \", end=\"\", flush=True)\n",
    "\n",
    "    @override\n",
    "    def on_text_delta(self, delta, snapshot):\n",
    "        print(delta.value, end=\"\", flush=True)\n",
    "\n",
    "    def on_tool_call_created(self, tool_call):\n",
    "        print(f\"\\nassistant > {tool_call.type}\\n\", flush=True)\n",
    "\n",
    "    def on_tool_call_delta(self, delta, snapshot):\n",
    "        if delta.type == 'code_interpreter':\n",
    "            if delta.code_interpreter.input:\n",
    "                print(delta.code_interpreter.input, end=\"\", flush=True)\n",
    "            if delta.code_interpreter.outputs:\n",
    "                print(f\"\\n\\noutput >\", flush=True)\n",
    "                for output in delta.code_interpreter.outputs:\n",
    "                    if output.type == \"logs\":\n",
    "                        print(f\"\\n{output.logs}\", flush=True)\n",
    "\n",
    "# Asistan akışını başlat\n",
    "with client.beta.threads.runs.stream(\n",
    "    thread_id=thread.id,\n",
    "    assistant_id=assistant.id,\n",
    "    instructions=\"Write in a professional tone for a Turkish-speaking audience.\",\n",
    "    event_handler=EventHandler(),\n",
    ") as stream:\n",
    "    stream.until_done()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import bitsandbytes as bnb\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(\"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\") \n",
    "\n",
    "# Eğer GPU kullanıyorsan, GPU device'ını tanımla\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Tokenizer ve model yükle (Quantized)\n",
    "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-3.2-1B', trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-1B', device_map='auto', load_in_8bit=True, trust_remote_code=True)\n",
    "\n",
    "# Prompt oluşturma fonksiyonuna language parametresi ekliyoruz\n",
    "def create_prompt(company_info, keywords, topic, length, language=\"English\"):\n",
    "    prompt = f\"Write a {length}-word blog post in {language} about {topic} using the following keywords: {keywords}.\"\n",
    "    return prompt\n",
    "\n",
    "# Türkçe veya İngilizce dilde prompt oluşturma\n",
    "company_info = \".\"\n",
    "keywords = \"İnşaat Hak Ediş, İnşaat birim fiyat\"\n",
    "topic = \"Average Cost and Deserve\"\n",
    "length = 300\n",
    "\n",
    "# Türkçe dilinde blog yazısı istemek için language='Turkish' parametresini kullanıyoruz\n",
    "prompt = create_prompt(company_info, keywords, topic, length, language=\"Turkısh\")\n",
    "\n",
    "# Promptu tokenize et\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Modelden sonuç al (quantized model ile) ve uyarıları çözmek için do_sample=True ekliyoruz\n",
    "outputs = model.generate(**inputs, max_new_tokens=300, temperature=0.7, top_p=0.9, do_sample=True)\n",
    "\n",
    "# Çıktıyı çözümle ve yazdır (temiz çıktı)\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True , clean_up_tokenization_spaces= True)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Llama 3.1 - 70B Model\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "# Eğer GPU kullanıyorsan, GPU device'ını tanımla\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Tokenizer ve model yükle (Quantized)\n",
    "tokenizer = AutoTokenizer.from_pretrained('nvidia/Llama-3.1-Nemotron-70B-Instruct-HF', trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    'nvidia/Llama-3.1-Nemotron-70B-Instruct-HF',\n",
    "    device_map=\"auto\",\n",
    "    load_in_8bit=True,\n",
    "    max_memory={0: \"100GB\", \"cpu\": \"30GB\"},\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Prompt oluşturma fonksiyonuna language parametresi ekliyoruz\n",
    "def create_prompt(company_info, keywords, topic, length, language=\"English\"):\n",
    "    prompt = f\"Write a {length}-word blog post in {language} about {topic} using the following keywords: {keywords}.\"\n",
    "    return prompt\n",
    "\n",
    "# Türkçe veya İngilizce dilde prompt oluşturma\n",
    "company_info = \".\"\n",
    "keywords = \"İnşaat Hak Ediş, İnşaat birim fiyat\"\n",
    "topic = \"Average Cost and Deserve\"\n",
    "length = 300\n",
    "\n",
    "# Türkçe dilinde blog yazısı istemek için language='Turkish' parametresini kullanıyoruz\n",
    "prompt = create_prompt(company_info, keywords, topic, length, language=\"Turkish\")\n",
    "\n",
    "# Promptu tokenize et\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Modelden sonuç al (quantized model ile) ve uyarıları çözmek için do_sample=True ekliyoruz\n",
    "outputs = model.generate(**inputs, max_new_tokens=300, temperature=0.7, top_p=0.9, do_sample=True)\n",
    "\n",
    "# Çıktıyı çözümle ve yazdır (temiz çıktı)\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title SuperNova - Medius Model\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "# Eğer GPU kullanıyorsan, GPU device'ını tanımla\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Tokenizer ve model yükle (Quantized)\n",
    "tokenizer = AutoTokenizer.from_pretrained('arcee-ai/SuperNova-Medius', trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained('arcee-ai/SuperNova-Medius', device_map='auto', load_in_8bit=True, trust_remote_code=True)\n",
    "\n",
    "# Prompt oluşturma fonksiyonuna language parametresi ekliyoruz\n",
    "def create_prompt(company_info, keywords, topic, length, language=\"English\"):\n",
    "    prompt = f\"Write a {length}-word blog post in {language} about {topic} using the following keywords: {keywords}.\"\n",
    "    return prompt\n",
    "\n",
    "# Türkçe veya İngilizce dilde prompt oluşturma\n",
    "company_info = \".\"\n",
    "keywords = \"İnşaat Hak Ediş, İnşaat birim fiyat\"\n",
    "topic = \"Average Cost and Deserve\"\n",
    "length = 300\n",
    "\n",
    "# Türkçe dilinde blog yazısı istemek için language='Turkish' parametresini kullanıyoruz\n",
    "prompt = create_prompt(company_info, keywords, topic, length, language=\"Turkısh\")\n",
    "\n",
    "# Promptu tokenize et\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Modelden sonuç al (quantized model ile) ve uyarıları çözmek için do_sample=True ekliyoruz\n",
    "outputs = model.generate(**inputs, max_new_tokens=300, temperature=0.7, top_p=0.9, do_sample=True)\n",
    "\n",
    "# Çıktıyı çözümle ve yazdır (temiz çıktı)\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True , clean_up_tokenization_spaces= True)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
